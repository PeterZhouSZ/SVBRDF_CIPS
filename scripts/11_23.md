
To train on bricks dataset: 

a. To run on 512x512 data and generate 512x512 images directly. `no crop` and we decrease the latent dim and style dim to `64`

> python -m torch.distributed.launch --nproc_per_node=8 --master_port=1234 train.py --n_sample=2 --batch=2 --fid_batch=8 --Generator=CIPSskip --output_dir=Bricks512_64zw --img2dis --num_workers=16 DATASET_PATH --img_c 5 --coords_size 512 --size 512 --crop_size 512 --N_emb 10 --latent 64 --fc_dim 64

a. start from 512x512 coordinates, random crop `at the beginning` as well as `dataset` to get 256x256 patch. and we decrease the latent dim and style dim to `64`

> python -m torch.distributed.launch --nproc_per_node=8 --master_port=1234 train.py --n_sample=2 --batch=2 --fid_batch=8 --Generator=CIPSskip --output_dir=Bricks512cb_64zw --img2dis --num_workers=16 DATASET_PATH --img_c 5 --coords_size 512 --size 256 --crop_size 256 --N_emb 10 --latent 64 --fc_dim 64