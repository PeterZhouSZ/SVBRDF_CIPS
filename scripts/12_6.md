
To train on bricks dataset with input patterns: 

for the dataset path: after unzipping the folder. the default should be `./Data/Bricks_inpat2/512`

All of these are: `tileable crop 256 from 512 at the beginning`

a. fed blurred pattern to top:

> python -m torch.distributed.launch --nproc_per_node=4 --master_port=1234 train.py --n_sample=2 --batch=2 --Generator=CIPSskip --output_dir=pat8_top --img2dis --num_workers=4 ./Data/Bricks_pat2/512 --img_c 5 --coords_size 512 --size 256 --crop_size 256 --N_emb 10 --latent 64 --fc_dim 64 --emb_pat blur --in_pat top --in_pat_c 8


b. fed blurred pattern to all layers:

> python -m torch.distributed.launch --nproc_per_node=4 --master_port=1234 train.py --n_sample=2 --batch=2 --Generator=CIPSskip --output_dir=pat8_all --img2dis --num_workers=4 ./Data/Bricks_pat2/512 --img_c 5 --coords_size 512 --size 256 --crop_size 256 --N_emb 10 --latent 64 --fc_dim 64 --emb_pat blur --in_pat all --in_pat_c 8

c. fed network trained pattern to top:

> python -m torch.distributed.launch --nproc_per_node=4 --master_port=1234 train.py --n_sample=2 --batch=2 --Generator=CIPSskip --output_dir=netpat32_top --img2dis --num_workers=4 ./Data/Bricks_pat2/512 --img_c 5 --coords_size 512 --size 256 --crop_size 256 --N_emb 10 --latent 64 --fc_dim 64 --emb_pat net_conv --in_pat top --in_pat_c 32

d. fed network trained patterns to all layers:

> python -m torch.distributed.launch --nproc_per_node=4 --master_port=1234 train.py --n_sample=2 --batch=2 --Generator=CIPSskip --output_dir=netpat32_all --img2dis --num_workers=4 ./Data/Bricks_pat2/512 --img_c 5 --coords_size 512 --size 256 --crop_size 256 --N_emb 10 --latent 64 --fc_dim 64 --emb_pat net_conv --in_pat all --in_pat_c 32
